https://lilianweng.github.io/posts/2018-06-24-attention/
https://paperswithcode.com/method/scaled
https://theaisummer.com/transformer/



self-attention: https://theaisummer.com/self-attention/ 

MHSA: https://paperswithcode.com/method/multi-head-attention 
https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/

