https://lilianweng.github.io/posts/2018-06-24-attention/ - READ 

https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960
https://paperswithcode.com/method/scaled
https://www.google.com/search?q=o+que+e+matriz+transposta&oq=o+que+e+uma+matriz+trans&aqs=chrome.1.69i57j0i22i30l2j69i64.5629j0j7&sourceid=chrome&ie=UTF-8
https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/



https://theaisummer.com/transformer/



self-attention: https://theaisummer.com/self-attention/ 

MHSA: https://paperswithcode.com/method/multi-head-attention 
https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/

